{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1vHOxLS8yqd"
      },
      "source": [
        "# Training a `torch` model\n",
        "---\n",
        "\n",
        "This notebook provides a simple implementation of a training loop for training a neural network with Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RFPRWBd9Rfq",
        "outputId": "1d02c3c6-9ba9-4e1e-f1d1-955696b9a106"
      },
      "outputs": [],
      "source": [
        "import pkg_resources\n",
        "\n",
        "run_colab = \"google\" in {pkg.key for pkg in pkg_resources.working_set}\n",
        "\n",
        "\n",
        "if run_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    root = \"/content/drive/MyDrive/DatSciEO\"\n",
        "    import sys\n",
        "    sys.path.append(root)\n",
        "else:\n",
        "    root = \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B0FTSShZ8yqh"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from models import TreeClassifConvNet\n",
        "from utils import TreeClassifPreprocessedDataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3lgKfOc8yqk"
      },
      "source": [
        "---\n",
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RMrErzMk8yqk"
      },
      "outputs": [],
      "source": [
        "# general training settings\n",
        "N_epochs = 400\n",
        "batch_size = 18\n",
        "learning_rate = 1e-4\n",
        "verbose = True\n",
        "\n",
        "# where to save training progress info and checkpoints\n",
        "run_dir = os.path.join(root, f\"runs/{time.strftime('%Y%m%d-%Hh%Mm%Ss', time.localtime())}\")\n",
        "checkpoint_dir = os.path.join(run_dir, \"checkpoints\")\n",
        "\n",
        "os.makedirs(run_dir, exist_ok=True)\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QPsNuqtb8yqn",
        "outputId": "c287783f-e15e-485f-e569-f9e1257206ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using dataset with properties:\n",
            "\tsamples:    10986\n",
            "\t   train:   7691\n",
            "\t   val:     3295\n",
            "\tshape: (30, 5, 5)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create datasets and dataloaders\n",
        "dataset_dir = os.path.join(root, \"data/1123_top10/1123_delete_nan_samples_\")\n",
        "\n",
        "dataset = TreeClassifPreprocessedDataset(\n",
        "    dataset_dir,\n",
        "    torchify=True\n",
        "    )\n",
        "\n",
        "# split the dataset into train and validation\n",
        "splits = [.7, .3]\n",
        "ds_train, ds_val = random_split(dataset, splits, generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# define dataloaders for training\n",
        "dl_train = DataLoader(ds_train, batch_size, shuffle=True)\n",
        "dl_val = DataLoader(ds_val, batch_size, shuffle=True)\n",
        "\n",
        "if verbose: print(\n",
        "    f\"\\nUsing dataset with properties:\\n\"       \\\n",
        "    f\"\\tsamples:    {len(dataset)}\\n\"              \\\n",
        "    f\"\\t   train:   {len(ds_train)}\\n\"             \\\n",
        "    f\"\\t   val:     {len(ds_val)}\\n\"               \\\n",
        "    f\"\\tshape: {dataset[0][0].shape}\\n\"         \\\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tfOcOpVY8yqo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TreeClassifConvNet(\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(30, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(15, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(7, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): ReLU()\n",
              "    (6): Flatten(start_dim=1, end_dim=-1)\n",
              "    (7): Linear(in_features=125, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model, loss and optimizer\n",
        "model = TreeClassifConvNet(\n",
        "    n_classes = dataset.n_classes,\n",
        "    width = dataset.width,\n",
        "    height = dataset.height,\n",
        "    depth = dataset.depth\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlXiu9Pd8yqq"
      },
      "source": [
        "---\n",
        "## Training\n",
        "### Sanity Check\n",
        "One final step before running the time-consuming loop is to perform a sanity check:\n",
        "- is the GPU working?\n",
        "- is the model able to learn? -> overfitting experiment on a small subset\n",
        "\n",
        "<br>\n",
        "Check whether the current device set to the GPU if available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2M0F20A28yqr",
        "outputId": "c4cbf5cb-09c5-403d-8ef2-9a011c9a33e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cpu\n",
            "model:  cpu\n"
          ]
        }
      ],
      "source": [
        "# device check\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "print(\"device:\", device)\n",
        "print(\"model: \", next(model.parameters()).device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOuy6b5T8yqu"
      },
      "source": [
        "Run an overfitting experiment with a small subset of the data.\n",
        "The training loss should go to zero and the validation loss should explode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rAANdohC8yqv",
        "outputId": "fe152fa3-346a-4a4d-de2c-748f20b02642"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# save initial model state to recover after dummy training\n",
        "checkpoint = {'model': model.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, 'checkpoint.pth')\n",
        "\n",
        "# define some small datasets from subsets of the data\n",
        "ds_train_check = TreeClassifPreprocessedDataset(dataset_dir, torchify=True, indices=[*range(50)])\n",
        "# choose samples from the end of the dataset for validation to ensure class dissimilarity\n",
        "ds_val_check = TreeClassifPreprocessedDataset(dataset_dir, torchify=True, indices=[*range(len(dataset)-42, len(dataset))])\n",
        "\n",
        "\n",
        "dl_train_check = DataLoader(ds_train_check, batch_size, shuffle=True)\n",
        "dl_val_check = DataLoader(ds_val_check, batch_size, shuffle=True)\n",
        "n_train_check, n_val_check = len(ds_train_check), len(ds_val_check)\n",
        "\n",
        "# train a few epochs and observe training and validation error\n",
        "losses_train = []\n",
        "losses_val = []\n",
        "\n",
        "for i_epoch in range(150):\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.\n",
        "    for i_batch, (x, y) in enumerate(dl_train_check):\n",
        "        assert torch.isnan(x).sum() == 0, \"NaN in trainings data, please fix.\"\n",
        "\n",
        "        x = x.float().to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    running_loss_val = 0.\n",
        "    with torch.no_grad():\n",
        "        for i_batch_val, (x, y) in enumerate(dl_val_check):\n",
        "            x = x.float().to(device)\n",
        "            y = y.to(device)\n",
        "            \n",
        "            pred_val = model(x.float().to(device))\n",
        "\n",
        "            loss_val = loss_fn(pred_val, y)\n",
        "            running_loss_val += loss_val.item()\n",
        "\n",
        "    losses_train.append(running_loss / n_train_check)\n",
        "    losses_val.append(running_loss_val / n_val_check)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.title(\"Training and Validation Loss \")\n",
        "plt.plot(losses_train, label=\"train\")\n",
        "plt.plot(losses_val, label=\"val\")\n",
        "plt.legend()\n",
        "\n",
        "# after dummy training, restore initial state\n",
        "checkpoint = torch.load(\"checkpoint.pth\")\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "optim.load_state_dict(checkpoint['optimizer'])\n",
        "os.remove(\"checkpoint.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RoyTA7y8yqy"
      },
      "source": [
        "### Training Loop\n",
        "After defining all the necessary parameters above, we can finally run the training loop.\n",
        "In the real training loop, for each epoch following metrics are calculated:\n",
        "- average running training loss over the whole epoch\n",
        "- average validation loss after epoch\n",
        "- training accuracy after epoch\n",
        "- validation accuracy after epoch\n",
        "\n",
        "The metrics are also logged with the help of the `tensorboard` package and can be viewed by...\n",
        "- running `tensorboard --logdir=<YOUR_LOG_DIR>`\n",
        "- then visiting [http://localhost:6006/](http://localhost:6006/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ek151zoz8yq0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train:  2.292817  loss avg: 2.292817   [epoch   0:    18/ 7691]\t\tt/it 0.17\tETA 3:05:43.780457\n",
            "train:  2.298388  loss avg: 2.295602   [epoch   0:    36/ 7691]\t\tt/it 0.14\tETA 2:34:26.311167\n",
            "train:  2.295323  loss avg: 2.295509   [epoch   0:    54/ 7691]\t\tt/it 0.14\tETA 2:30:22.738958\n",
            "train:  2.306276  loss avg: 2.298201   [epoch   0:    72/ 7691]\t\tt/it 0.17\tETA 2:59:27.482433\n",
            "train:  2.301334  loss avg: 2.298828   [epoch   0:    90/ 7691]\t\tt/it 0.16\tETA 2:49:47.993760\n",
            "train:  2.297490  loss avg: 2.298605   [epoch   0:   108/ 7691]\t\tt/it 0.15\tETA 2:43:11.311511\n",
            "train:  2.297935  loss avg: 2.298509   [epoch   0:   126/ 7691]\t\tt/it 0.15\tETA 2:36:13.247832\n",
            "train:  2.292741  loss avg: 2.297788   [epoch   0:   144/ 7691]\t\tt/it 0.14\tETA 2:30:03.570623\n",
            "train:  2.295990  loss avg: 2.297588   [epoch   0:   162/ 7691]\t\tt/it 0.16\tETA 2:49:04.522473\n",
            "train:  2.314788  loss avg: 2.299308   [epoch   0:   180/ 7691]\t\tt/it 0.16\tETA 2:47:57.665818\n",
            "train:  2.298724  loss avg: 2.299255   [epoch   0:   198/ 7691]\t\tt/it 0.15\tETA 2:44:44.567616\n",
            "train:  2.296914  loss avg: 2.299060   [epoch   0:   216/ 7691]\t\tt/it 0.16\tETA 2:52:27.649315\n",
            "train:  2.286488  loss avg: 2.298093   [epoch   0:   234/ 7691]\t\tt/it 0.16\tETA 2:48:02.633409\n",
            "train:  2.297012  loss avg: 2.298016   [epoch   0:   252/ 7691]\t\tt/it 0.15\tETA 2:45:32.739103\n",
            "train:  2.295802  loss avg: 2.297868   [epoch   0:   270/ 7691]\t\tt/it 0.15\tETA 2:42:48.760070\n",
            "train:  2.287198  loss avg: 2.297201   [epoch   0:   288/ 7691]\t\tt/it 0.16\tETA 2:48:10.029494\n",
            "train:  2.292000  loss avg: 2.296895   [epoch   0:   306/ 7691]\t\tt/it 0.16\tETA 2:46:30.554635\n",
            "train:  2.303147  loss avg: 2.297243   [epoch   0:   324/ 7691]\t\tt/it 0.15\tETA 2:43:41.092094\n",
            "train:  2.300698  loss avg: 2.297424   [epoch   0:   342/ 7691]\t\tt/it 0.15\tETA 2:41:21.680165\n",
            "train:  2.289598  loss avg: 2.297033   [epoch   0:   360/ 7691]\t\tt/it 0.15\tETA 2:38:36.858332\n",
            "train:  2.299926  loss avg: 2.297171   [epoch   0:   378/ 7691]\t\tt/it 0.15\tETA 2:43:08.516109\n",
            "train:  2.290771  loss avg: 2.296880   [epoch   0:   396/ 7691]\t\tt/it 0.15\tETA 2:41:13.155779\n",
            "train:  2.292398  loss avg: 2.296685   [epoch   0:   414/ 7691]\t\tt/it 0.15\tETA 2:38:34.562859\n",
            "train:  2.287509  loss avg: 2.296303   [epoch   0:   432/ 7691]\t\tt/it 0.15\tETA 2:36:40.499461\n",
            "train:  2.285606  loss avg: 2.295875   [epoch   0:   450/ 7691]\t\tt/it 0.14\tETA 2:34:32.185741\n",
            "train:  2.292003  loss avg: 2.295726   [epoch   0:   468/ 7691]\t\tt/it 0.15\tETA 2:38:50.557675\n",
            "train:  2.294031  loss avg: 2.295663   [epoch   0:   486/ 7691]\t\tt/it 0.15\tETA 2:36:45.096708\n",
            "train:  2.306062  loss avg: 2.296035   [epoch   0:   504/ 7691]\t\tt/it 0.15\tETA 2:35:08.735311\n",
            "train:  2.291338  loss avg: 2.295873   [epoch   0:   522/ 7691]\t\tt/it 0.14\tETA 2:33:25.121372\n",
            "train:  2.297033  loss avg: 2.295911   [epoch   0:   540/ 7691]\t\tt/it 0.15\tETA 2:36:09.451087\n",
            "train:  2.282392  loss avg: 2.295475   [epoch   0:   558/ 7691]\t\tt/it 0.14\tETA 2:34:28.728601\n",
            "train:  2.297465  loss avg: 2.295537   [epoch   0:   576/ 7691]\t\tt/it 0.14\tETA 2:32:59.451847\n",
            "train:  2.291295  loss avg: 2.295409   [epoch   0:   594/ 7691]\t\tt/it 0.14\tETA 2:32:40.629415\n",
            "train:  2.285805  loss avg: 2.295126   [epoch   0:   612/ 7691]\t\tt/it 0.14\tETA 2:31:26.690686\n",
            "train:  2.287518  loss avg: 2.294909   [epoch   0:   630/ 7691]\t\tt/it 0.14\tETA 2:33:44.767765\n",
            "train:  2.283628  loss avg: 2.294596   [epoch   0:   648/ 7691]\t\tt/it 0.14\tETA 2:32:24.241759\n",
            "train:  2.277968  loss avg: 2.294146   [epoch   0:   666/ 7691]\t\tt/it 0.14\tETA 2:31:12.798833\n",
            "train:  2.294614  loss avg: 2.294159   [epoch   0:   684/ 7691]\t\tt/it 0.14\tETA 2:29:57.358560\n",
            "train:  2.285451  loss avg: 2.293935   [epoch   0:   702/ 7691]\t\tt/it 0.14\tETA 2:28:56.253559\n",
            "train:  2.277799  loss avg: 2.293532   [epoch   0:   720/ 7691]\t\tt/it 0.14\tETA 2:31:15.676739\n",
            "train:  2.288080  loss avg: 2.293399   [epoch   0:   738/ 7691]\t\tt/it 0.14\tETA 2:30:00.321311\n",
            "train:  2.297275  loss avg: 2.293491   [epoch   0:   756/ 7691]\t\tt/it 0.14\tETA 2:29:05.893789\n",
            "train:  2.274908  loss avg: 2.293059   [epoch   0:   774/ 7691]\t\tt/it 0.14\tETA 2:27:54.236950\n",
            "train:  2.268674  loss avg: 2.292505   [epoch   0:   792/ 7691]\t\tt/it 0.14\tETA 2:26:59.840724\n",
            "train:  2.268905  loss avg: 2.291980   [epoch   0:   810/ 7691]\t\tt/it 0.14\tETA 2:28:33.474116\n",
            "train:  2.266366  loss avg: 2.291424   [epoch   0:   828/ 7691]\t\tt/it 0.14\tETA 2:27:32.407491\n",
            "train:  2.282660  loss avg: 2.291237   [epoch   0:   846/ 7691]\t\tt/it 0.14\tETA 2:26:38.395103\n",
            "train:  2.290264  loss avg: 2.291217   [epoch   0:   864/ 7691]\t\tt/it 0.14\tETA 2:25:34.647382\n",
            "train:  2.295761  loss avg: 2.291310   [epoch   0:   882/ 7691]\t\tt/it 0.14\tETA 2:24:40.486113\n",
            "train:  2.291803  loss avg: 2.291319   [epoch   0:   900/ 7691]\t\tt/it 0.14\tETA 2:25:37.902275\n",
            "train:  2.283821  loss avg: 2.291172   [epoch   0:   918/ 7691]\t\tt/it 0.14\tETA 2:24:49.654005\n",
            "train:  2.262516  loss avg: 2.290621   [epoch   0:   936/ 7691]\t\tt/it 0.14\tETA 2:24:43.935204\n",
            "train:  2.285799  loss avg: 2.290530   [epoch   0:   954/ 7691]\t\tt/it 0.13\tETA 2:23:56.596025\n",
            "train:  2.279262  loss avg: 2.290322   [epoch   0:   972/ 7691]\t\tt/it 0.13\tETA 2:23:11.987636\n",
            "train:  2.265162  loss avg: 2.289864   [epoch   0:   990/ 7691]\t\tt/it 0.14\tETA 2:24:21.930678\n",
            "train:  2.270797  loss avg: 2.289524   [epoch   0:  1008/ 7691]\t\tt/it 0.13\tETA 2:23:31.799404\n",
            "train:  2.282754  loss avg: 2.289405   [epoch   0:  1026/ 7691]\t\tt/it 0.13\tETA 2:22:55.942883\n",
            "train:  2.281167  loss avg: 2.289263   [epoch   0:  1044/ 7691]\t\tt/it 0.13\tETA 2:22:12.998216\n",
            "train:  2.285903  loss avg: 2.289206   [epoch   0:  1062/ 7691]\t\tt/it 0.13\tETA 2:21:34.103197\n",
            "train:  2.295711  loss avg: 2.289314   [epoch   0:  1080/ 7691]\t\tt/it 0.13\tETA 2:22:27.768657\n",
            "train:  2.255731  loss avg: 2.288764   [epoch   0:  1098/ 7691]\t\tt/it 0.13\tETA 2:21:39.257032\n",
            "train:  2.286751  loss avg: 2.288731   [epoch   0:  1116/ 7691]\t\tt/it 0.13\tETA 2:20:48.404156\n",
            "train:  2.274027  loss avg: 2.288498   [epoch   0:  1134/ 7691]\t\tt/it 0.13\tETA 2:20:17.821733\n",
            "train:  2.264810  loss avg: 2.288128   [epoch   0:  1152/ 7691]\t\tt/it 0.13\tETA 2:20:41.294155\n",
            "train:  2.264451  loss avg: 2.287764   [epoch   0:  1170/ 7691]\t\tt/it 0.13\tETA 2:20:22.234293\n",
            "train:  2.257656  loss avg: 2.287307   [epoch   0:  1188/ 7691]\t\tt/it 0.13\tETA 2:19:52.512220\n",
            "train:  2.277314  loss avg: 2.287158   [epoch   0:  1206/ 7691]\t\tt/it 0.13\tETA 2:19:21.531639\n",
            "train:  2.266106  loss avg: 2.286849   [epoch   0:  1224/ 7691]\t\tt/it 0.13\tETA 2:19:08.033543\n",
            "train:  2.258627  loss avg: 2.286440   [epoch   0:  1242/ 7691]\t\tt/it 0.13\tETA 2:19:53.024968\n",
            "train:  2.293188  loss avg: 2.286536   [epoch   0:  1260/ 7691]\t\tt/it 0.13\tETA 2:19:28.698294\n",
            "train:  2.286324  loss avg: 2.286533   [epoch   0:  1278/ 7691]\t\tt/it 0.13\tETA 2:19:11.884056\n",
            "train:  2.280116  loss avg: 2.286444   [epoch   0:  1296/ 7691]\t\tt/it 0.13\tETA 2:18:44.780501\n",
            "train:  2.264096  loss avg: 2.286138   [epoch   0:  1314/ 7691]\t\tt/it 0.13\tETA 2:18:40.320142\n",
            "train:  2.253346  loss avg: 2.285695   [epoch   0:  1332/ 7691]\t\tt/it 0.13\tETA 2:19:38.656821\n",
            "train:  2.274029  loss avg: 2.285539   [epoch   0:  1350/ 7691]\t\tt/it 0.13\tETA 2:18:58.661724\n",
            "train:  2.263212  loss avg: 2.285245   [epoch   0:  1368/ 7691]\t\tt/it 0.13\tETA 2:18:31.847422\n",
            "train:  2.285218  loss avg: 2.285245   [epoch   0:  1386/ 7691]\t\tt/it 0.13\tETA 2:18:16.510720\n",
            "train:  2.242804  loss avg: 2.284701   [epoch   0:  1404/ 7691]\t\tt/it 0.13\tETA 2:18:39.332912\n",
            "train:  2.265956  loss avg: 2.284464   [epoch   0:  1422/ 7691]\t\tt/it 0.13\tETA 2:18:08.036812\n",
            "train:  2.242934  loss avg: 2.283944   [epoch   0:  1440/ 7691]\t\tt/it 0.13\tETA 2:17:40.271416\n",
            "train:  2.270145  loss avg: 2.283774   [epoch   0:  1458/ 7691]\t\tt/it 0.13\tETA 2:17:11.389513\n",
            "train:  2.261476  loss avg: 2.283502   [epoch   0:  1476/ 7691]\t\tt/it 0.13\tETA 2:16:49.060393\n",
            "train:  2.258585  loss avg: 2.283202   [epoch   0:  1494/ 7691]\t\tt/it 0.13\tETA 2:17:10.294143\n",
            "train:  2.227636  loss avg: 2.282540   [epoch   0:  1512/ 7691]\t\tt/it 0.13\tETA 2:16:49.967052\n",
            "train:  2.261866  loss avg: 2.282297   [epoch   0:  1530/ 7691]\t\tt/it 0.13\tETA 2:16:29.585687\n",
            "train:  2.236166  loss avg: 2.281761   [epoch   0:  1548/ 7691]\t\tt/it 0.13\tETA 2:15:55.919114\n",
            "train:  2.220127  loss avg: 2.281052   [epoch   0:  1566/ 7691]\t\tt/it 0.13\tETA 2:15:36.055230\n",
            "train:  2.231329  loss avg: 2.280487   [epoch   0:  1584/ 7691]\t\tt/it 0.13\tETA 2:16:05.193491\n",
            "train:  2.254441  loss avg: 2.280195   [epoch   0:  1602/ 7691]\t\tt/it 0.13\tETA 2:15:41.295621\n",
            "train:  2.259723  loss avg: 2.279967   [epoch   0:  1620/ 7691]\t\tt/it 0.13\tETA 2:15:13.746097\n",
            "train:  2.275287  loss avg: 2.279916   [epoch   0:  1638/ 7691]\t\tt/it 0.13\tETA 2:14:52.450609\n",
            "train:  2.251811  loss avg: 2.279610   [epoch   0:  1656/ 7691]\t\tt/it 0.13\tETA 2:14:25.472746\n",
            "train:  2.201617  loss avg: 2.278772   [epoch   0:  1674/ 7691]\t\tt/it 0.13\tETA 2:14:49.350296\n",
            "train:  2.215809  loss avg: 2.278102   [epoch   0:  1692/ 7691]\t\tt/it 0.13\tETA 2:14:22.672801\n",
            "train:  2.254388  loss avg: 2.277852   [epoch   0:  1710/ 7691]\t\tt/it 0.13\tETA 2:13:54.355195\n",
            "train:  2.235818  loss avg: 2.277414   [epoch   0:  1728/ 7691]\t\tt/it 0.13\tETA 2:13:38.559245\n",
            "train:  2.219945  loss avg: 2.276822   [epoch   0:  1746/ 7691]\t\tt/it 0.12\tETA 2:13:16.621713\n",
            "train:  2.251235  loss avg: 2.276561   [epoch   0:  1764/ 7691]\t\tt/it 0.13\tETA 2:13:46.781608\n",
            "train:  2.239254  loss avg: 2.276184   [epoch   0:  1782/ 7691]\t\tt/it 0.12\tETA 2:13:21.920971\n",
            "train:  2.243192  loss avg: 2.275854   [epoch   0:  1800/ 7691]\t\tt/it 0.12\tETA 2:13:06.361199\n",
            "train:  2.240881  loss avg: 2.275508   [epoch   0:  1818/ 7691]\t\tt/it 0.12\tETA 2:12:47.555677\n",
            "train:  2.257094  loss avg: 2.275327   [epoch   0:  1836/ 7691]\t\tt/it 0.12\tETA 2:13:16.889338\n",
            "train:  2.228365  loss avg: 2.274871   [epoch   0:  1854/ 7691]\t\tt/it 0.12\tETA 2:13:11.043078\n",
            "train:  2.263556  loss avg: 2.274763   [epoch   0:  1872/ 7691]\t\tt/it 0.12\tETA 2:12:58.584849\n",
            "train:  2.229222  loss avg: 2.274329   [epoch   0:  1890/ 7691]\t\tt/it 0.12\tETA 2:12:44.050937\n",
            "train:  2.236495  loss avg: 2.273972   [epoch   0:  1908/ 7691]\t\tt/it 0.12\tETA 2:12:26.705570\n",
            "train:  2.214479  loss avg: 2.273416   [epoch   0:  1926/ 7691]\t\tt/it 0.12\tETA 2:13:00.224337\n",
            "train:  2.197858  loss avg: 2.272716   [epoch   0:  1944/ 7691]\t\tt/it 0.12\tETA 2:12:44.327840\n",
            "train:  2.239793  loss avg: 2.272414   [epoch   0:  1962/ 7691]\t\tt/it 0.12\tETA 2:12:30.174554\n",
            "train:  2.158629  loss avg: 2.271380   [epoch   0:  1980/ 7691]\t\tt/it 0.12\tETA 2:12:09.882269\n",
            "train:  2.202487  loss avg: 2.270759   [epoch   0:  1998/ 7691]\t\tt/it 0.12\tETA 2:11:57.002489\n",
            "train:  2.210205  loss avg: 2.270218   [epoch   0:  2016/ 7691]\t\tt/it 0.12\tETA 2:12:17.750284\n",
            "train:  2.188257  loss avg: 2.269493   [epoch   0:  2034/ 7691]\t\tt/it 0.12\tETA 2:11:58.414769\n",
            "train:  2.160384  loss avg: 2.268536   [epoch   0:  2052/ 7691]\t\tt/it 0.12\tETA 2:11:45.402759\n",
            "train:  2.167109  loss avg: 2.267654   [epoch   0:  2070/ 7691]\t\tt/it 0.12\tETA 2:11:34.966011\n",
            "train:  2.148883  loss avg: 2.266630   [epoch   0:  2088/ 7691]\t\tt/it 0.12\tETA 2:11:23.945565\n",
            "train:  2.243135  loss avg: 2.266429   [epoch   0:  2106/ 7691]\t\tt/it 0.12\tETA 2:11:43.876783\n",
            "train:  2.194349  loss avg: 2.265819   [epoch   0:  2124/ 7691]\t\tt/it 0.12\tETA 2:11:31.075586\n",
            "train:  2.236659  loss avg: 2.265573   [epoch   0:  2142/ 7691]\t\tt/it 0.12\tETA 2:11:19.071119\n",
            "train:  2.152918  loss avg: 2.264635   [epoch   0:  2160/ 7691]\t\tt/it 0.12\tETA 2:11:17.060316\n",
            "train:  2.208455  loss avg: 2.264170   [epoch   0:  2178/ 7691]\t\tt/it 0.12\tETA 2:11:00.354534\n",
            "train:  2.210013  loss avg: 2.263726   [epoch   0:  2196/ 7691]\t\tt/it 0.12\tETA 2:11:15.810655\n",
            "train:  2.174204  loss avg: 2.262999   [epoch   0:  2214/ 7691]\t\tt/it 0.12\tETA 2:10:59.714452\n",
            "train:  2.150956  loss avg: 2.262095   [epoch   0:  2232/ 7691]\t\tt/it 0.12\tETA 2:10:47.920816\n",
            "train:  2.269856  loss avg: 2.262157   [epoch   0:  2250/ 7691]\t\tt/it 0.12\tETA 2:10:33.518525\n",
            "train:  2.164247  loss avg: 2.261380   [epoch   0:  2268/ 7691]\t\tt/it 0.12\tETA 2:10:49.185000\n",
            "train:  2.107100  loss avg: 2.260165   [epoch   0:  2286/ 7691]\t\tt/it 0.12\tETA 2:10:34.457483\n",
            "train:  2.104642  loss avg: 2.258950   [epoch   0:  2304/ 7691]\t\tt/it 0.12\tETA 2:10:15.415577\n",
            "train:  2.072854  loss avg: 2.257508   [epoch   0:  2322/ 7691]\t\tt/it 0.12\tETA 2:10:05.762083\n",
            "train:  2.160246  loss avg: 2.256760   [epoch   0:  2340/ 7691]\t\tt/it 0.12\tETA 2:09:54.300144\n",
            "train:  2.196239  loss avg: 2.256298   [epoch   0:  2358/ 7691]\t\tt/it 0.12\tETA 2:10:09.306440\n",
            "train:  2.066164  loss avg: 2.254857   [epoch   0:  2376/ 7691]\t\tt/it 0.12\tETA 2:09:56.451734\n",
            "train:  2.094261  loss avg: 2.253650   [epoch   0:  2394/ 7691]\t\tt/it 0.12\tETA 2:09:43.558051\n",
            "train:  1.996753  loss avg: 2.251733   [epoch   0:  2412/ 7691]\t\tt/it 0.12\tETA 2:09:29.060518\n",
            "train:  2.243987  loss avg: 2.251675   [epoch   0:  2430/ 7691]\t\tt/it 0.12\tETA 2:09:17.946786\n",
            "train:  2.067414  loss avg: 2.250320   [epoch   0:  2448/ 7691]\t\tt/it 0.12\tETA 2:09:34.244961\n",
            "train:  2.147720  loss avg: 2.249571   [epoch   0:  2466/ 7691]\t\tt/it 0.12\tETA 2:09:21.116352\n",
            "train:  2.186839  loss avg: 2.249117   [epoch   0:  2484/ 7691]\t\tt/it 0.12\tETA 2:09:07.521332\n",
            "train:  2.171837  loss avg: 2.248561   [epoch   0:  2502/ 7691]\t\tt/it 0.12\tETA 2:08:55.919336\n",
            "train:  2.110104  loss avg: 2.247572   [epoch   0:  2520/ 7691]\t\tt/it 0.12\tETA 2:08:44.418654\n",
            "train:  2.038239  loss avg: 2.246087   [epoch   0:  2538/ 7691]\t\tt/it 0.12\tETA 2:09:13.584656\n",
            "train:  1.893749  loss avg: 2.243606   [epoch   0:  2556/ 7691]\t\tt/it 0.12\tETA 2:09:04.017083\n",
            "train:  2.216609  loss avg: 2.243417   [epoch   0:  2574/ 7691]\t\tt/it 0.12\tETA 2:08:55.125640\n",
            "train:  2.241568  loss avg: 2.243404   [epoch   0:  2592/ 7691]\t\tt/it 0.12\tETA 2:08:43.552148\n",
            "train:  2.124257  loss avg: 2.242583   [epoch   0:  2610/ 7691]\t\tt/it 0.12\tETA 2:08:29.393725\n",
            "train:  2.186385  loss avg: 2.242198   [epoch   0:  2628/ 7691]\t\tt/it 0.12\tETA 2:08:41.678561\n",
            "train:  1.983917  loss avg: 2.240441   [epoch   0:  2646/ 7691]\t\tt/it 0.12\tETA 2:08:29.420937\n",
            "train:  2.212533  loss avg: 2.240252   [epoch   0:  2664/ 7691]\t\tt/it 0.12\tETA 2:08:22.014822\n",
            "train:  2.029146  loss avg: 2.238835   [epoch   0:  2682/ 7691]\t\tt/it 0.12\tETA 2:08:07.765007\n",
            "train:  2.020658  loss avg: 2.237381   [epoch   0:  2700/ 7691]\t\tt/it 0.12\tETA 2:08:22.317670\n",
            "train:  1.964866  loss avg: 2.235576   [epoch   0:  2718/ 7691]\t\tt/it 0.12\tETA 2:08:12.029502\n",
            "train:  1.808329  loss avg: 2.232765   [epoch   0:  2736/ 7691]\t\tt/it 0.12\tETA 2:08:01.696615\n",
            "train:  2.339937  loss avg: 2.233466   [epoch   0:  2754/ 7691]\t\tt/it 0.12\tETA 2:07:48.100122\n",
            "train:  1.860038  loss avg: 2.231041   [epoch   0:  2772/ 7691]\t\tt/it 0.12\tETA 2:07:39.377689\n",
            "train:  1.923982  loss avg: 2.229060   [epoch   0:  2790/ 7691]\t\tt/it 0.12\tETA 2:07:51.919045\n",
            "train:  2.025010  loss avg: 2.227752   [epoch   0:  2808/ 7691]\t\tt/it 0.12\tETA 2:07:41.411094\n",
            "train:  2.102468  loss avg: 2.226954   [epoch   0:  2826/ 7691]\t\tt/it 0.12\tETA 2:07:29.830613\n",
            "train:  2.044753  loss avg: 2.225801   [epoch   0:  2844/ 7691]\t\tt/it 0.12\tETA 2:07:22.446917\n",
            "train:  1.985499  loss avg: 2.224289   [epoch   0:  2862/ 7691]\t\tt/it 0.12\tETA 2:07:07.581133\n",
            "train:  2.095338  loss avg: 2.223483   [epoch   0:  2880/ 7691]\t\tt/it 0.12\tETA 2:07:22.040691\n",
            "train:  2.153848  loss avg: 2.223051   [epoch   0:  2898/ 7691]\t\tt/it 0.12\tETA 2:07:11.448877\n",
            "train:  2.003489  loss avg: 2.221696   [epoch   0:  2916/ 7691]\t\tt/it 0.12\tETA 2:07:04.015085\n",
            "train:  2.243663  loss avg: 2.221830   [epoch   0:  2934/ 7691]\t\tt/it 0.12\tETA 2:06:50.688146\n",
            "train:  2.248519  loss avg: 2.221993   [epoch   0:  2952/ 7691]\t\tt/it 0.12\tETA 2:06:42.983287\n",
            "train:  1.928983  loss avg: 2.220217   [epoch   0:  2970/ 7691]\t\tt/it 0.12\tETA 2:06:57.131371\n",
            "train:  2.234502  loss avg: 2.220303   [epoch   0:  2988/ 7691]\t\tt/it 0.12\tETA 2:06:47.040333\n",
            "train:  2.203985  loss avg: 2.220206   [epoch   0:  3006/ 7691]\t\tt/it 0.12\tETA 2:06:46.771311\n",
            "train:  2.007550  loss avg: 2.218940   [epoch   0:  3024/ 7691]\t\tt/it 0.12\tETA 2:06:39.522241\n",
            "train:  2.187854  loss avg: 2.218756   [epoch   0:  3042/ 7691]\t\tt/it 0.12\tETA 2:06:28.154967\n",
            "train:  2.124309  loss avg: 2.218200   [epoch   0:  3060/ 7691]\t\tt/it 0.12\tETA 2:06:43.676456\n",
            "train:  2.115199  loss avg: 2.217598   [epoch   0:  3078/ 7691]\t\tt/it 0.12\tETA 2:06:32.165823\n",
            "train:  2.334208  loss avg: 2.218276   [epoch   0:  3096/ 7691]\t\tt/it 0.12\tETA 2:06:17.556789\n",
            "train:  2.126809  loss avg: 2.217747   [epoch   0:  3114/ 7691]\t\tt/it 0.12\tETA 2:06:11.218736\n",
            "train:  1.815726  loss avg: 2.215437   [epoch   0:  3132/ 7691]\t\tt/it 0.12\tETA 2:06:22.909695\n",
            "train:  2.004876  loss avg: 2.214233   [epoch   0:  3150/ 7691]\t\tt/it 0.12\tETA 2:06:13.426052\n",
            "train:  2.052356  loss avg: 2.213314   [epoch   0:  3168/ 7691]\t\tt/it 0.12\tETA 2:06:03.132471\n",
            "train:  1.806613  loss avg: 2.211016   [epoch   0:  3186/ 7691]\t\tt/it 0.12\tETA 2:05:49.789819\n",
            "train:  2.011333  loss avg: 2.209894   [epoch   0:  3204/ 7691]\t\tt/it 0.12\tETA 2:05:46.872037\n",
            "train:  2.010486  loss avg: 2.208780   [epoch   0:  3222/ 7691]\t\tt/it 0.12\tETA 2:06:00.354980\n",
            "train:  2.313093  loss avg: 2.209360   [epoch   0:  3240/ 7691]\t\tt/it 0.12\tETA 2:05:53.071166\n",
            "train:  1.898890  loss avg: 2.207644   [epoch   0:  3258/ 7691]\t\tt/it 0.12\tETA 2:05:44.253226\n",
            "train:  2.058124  loss avg: 2.206823   [epoch   0:  3276/ 7691]\t\tt/it 0.12\tETA 2:05:35.253490\n",
            "train:  2.008636  loss avg: 2.205740   [epoch   0:  3294/ 7691]\t\tt/it 0.12\tETA 2:05:27.948861\n",
            "train:  2.162272  loss avg: 2.205504   [epoch   0:  3312/ 7691]\t\tt/it 0.12\tETA 2:05:36.405148\n",
            "train:  1.943544  loss avg: 2.204088   [epoch   0:  3330/ 7691]\t\tt/it 0.12\tETA 2:05:27.367484\n",
            "train:  1.973348  loss avg: 2.202847   [epoch   0:  3348/ 7691]\t\tt/it 0.12\tETA 2:05:18.411733\n",
            "train:  2.002935  loss avg: 2.201778   [epoch   0:  3366/ 7691]\t\tt/it 0.12\tETA 2:05:15.336589\n",
            "train:  2.097824  loss avg: 2.201225   [epoch   0:  3384/ 7691]\t\tt/it 0.12\tETA 2:05:22.486485\n",
            "train:  1.988210  loss avg: 2.200098   [epoch   0:  3402/ 7691]\t\tt/it 0.12\tETA 2:05:50.362011\n",
            "train:  2.310757  loss avg: 2.200680   [epoch   0:  3420/ 7691]\t\tt/it 0.12\tETA 2:05:44.316658\n",
            "train:  1.901979  loss avg: 2.199117   [epoch   0:  3438/ 7691]\t\tt/it 0.12\tETA 2:05:41.384944\n",
            "train:  2.208961  loss avg: 2.199168   [epoch   0:  3456/ 7691]\t\tt/it 0.12\tETA 2:05:37.696782\n",
            "train:  2.168485  loss avg: 2.199009   [epoch   0:  3474/ 7691]\t\tt/it 0.12\tETA 2:05:33.758706\n",
            "train:  1.698108  loss avg: 2.196427   [epoch   0:  3492/ 7691]\t\tt/it 0.12\tETA 2:05:52.562621\n",
            "train:  2.029550  loss avg: 2.195571   [epoch   0:  3510/ 7691]\t\tt/it 0.12\tETA 2:05:46.579364\n",
            "train:  1.970180  loss avg: 2.194421   [epoch   0:  3528/ 7691]\t\tt/it 0.12\tETA 2:05:44.447718\n",
            "train:  2.216309  loss avg: 2.194532   [epoch   0:  3546/ 7691]\t\tt/it 0.12\tETA 2:05:44.012567\n",
            "train:  2.014891  loss avg: 2.193625   [epoch   0:  3564/ 7691]\t\tt/it 0.12\tETA 2:05:56.776583\n",
            "train:  2.115422  loss avg: 2.193232   [epoch   0:  3582/ 7691]\t\tt/it 0.12\tETA 2:05:51.874812\n",
            "train:  2.352828  loss avg: 2.194030   [epoch   0:  3600/ 7691]\t\tt/it 0.12\tETA 2:05:47.960892\n",
            "train:  1.918536  loss avg: 2.192659   [epoch   0:  3618/ 7691]\t\tt/it 0.12\tETA 2:05:47.625831\n",
            "train:  1.937195  loss avg: 2.191395   [epoch   0:  3636/ 7691]\t\tt/it 0.12\tETA 2:05:48.724488\n",
            "train:  1.937527  loss avg: 2.190144   [epoch   0:  3654/ 7691]\t\tt/it 0.12\tETA 2:06:00.257565\n",
            "train:  1.990703  loss avg: 2.189166   [epoch   0:  3672/ 7691]\t\tt/it 0.12\tETA 2:05:55.692276\n",
            "train:  2.276174  loss avg: 2.189591   [epoch   0:  3690/ 7691]\t\tt/it 0.12\tETA 2:05:49.948266\n",
            "train:  2.103818  loss avg: 2.189174   [epoch   0:  3708/ 7691]\t\tt/it 0.12\tETA 2:05:44.044930\n",
            "train:  2.223908  loss avg: 2.189342   [epoch   0:  3726/ 7691]\t\tt/it 0.12\tETA 2:05:41.589469\n",
            "train:  2.032750  loss avg: 2.188589   [epoch   0:  3744/ 7691]\t\tt/it 0.12\tETA 2:05:56.819113\n",
            "train:  1.995245  loss avg: 2.187664   [epoch   0:  3762/ 7691]\t\tt/it 0.12\tETA 2:05:51.232676\n",
            "train:  2.015581  loss avg: 2.186845   [epoch   0:  3780/ 7691]\t\tt/it 0.12\tETA 2:05:45.660711\n",
            "train:  1.960184  loss avg: 2.185771   [epoch   0:  3798/ 7691]\t\tt/it 0.12\tETA 2:05:48.670761\n",
            "train:  2.009338  loss avg: 2.184938   [epoch   0:  3816/ 7691]\t\tt/it 0.12\tETA 2:05:39.638097\n",
            "train:  2.494755  loss avg: 2.186393   [epoch   0:  3834/ 7691]\t\tt/it 0.12\tETA 2:05:52.239260\n",
            "train:  1.865886  loss avg: 2.184895   [epoch   0:  3852/ 7691]\t\tt/it 0.12\tETA 2:05:46.423314\n",
            "train:  2.420718  loss avg: 2.185992   [epoch   0:  3870/ 7691]\t\tt/it 0.12\tETA 2:05:39.493063\n",
            "train:  2.033780  loss avg: 2.185287   [epoch   0:  3888/ 7691]\t\tt/it 0.12\tETA 2:05:33.233123\n",
            "train:  1.932980  loss avg: 2.184125   [epoch   0:  3906/ 7691]\t\tt/it 0.12\tETA 2:05:27.397432\n",
            "train:  2.087724  loss avg: 2.183683   [epoch   0:  3924/ 7691]\t\tt/it 0.12\tETA 2:05:46.643859\n",
            "train:  2.334823  loss avg: 2.184373   [epoch   0:  3942/ 7691]\t\tt/it 0.12\tETA 2:05:46.215601\n",
            "train:  2.103261  loss avg: 2.184004   [epoch   0:  3960/ 7691]\t\tt/it 0.12\tETA 2:05:41.403256\n",
            "train:  2.201041  loss avg: 2.184081   [epoch   0:  3978/ 7691]\t\tt/it 0.12\tETA 2:05:38.754644\n",
            "train:  2.205278  loss avg: 2.184177   [epoch   0:  3996/ 7691]\t\tt/it 0.12\tETA 2:05:50.276504\n",
            "train:  1.882768  loss avg: 2.182825   [epoch   0:  4014/ 7691]\t\tt/it 0.12\tETA 2:05:45.354378\n",
            "train:  2.258166  loss avg: 2.183161   [epoch   0:  4032/ 7691]\t\tt/it 0.12\tETA 2:05:41.278874\n",
            "train:  2.273515  loss avg: 2.183563   [epoch   0:  4050/ 7691]\t\tt/it 0.12\tETA 2:05:39.863784\n",
            "train:  2.198192  loss avg: 2.183628   [epoch   0:  4068/ 7691]\t\tt/it 0.12\tETA 2:05:39.283045\n",
            "train:  2.020618  loss avg: 2.182909   [epoch   0:  4086/ 7691]\t\tt/it 0.12\tETA 2:05:51.587782\n",
            "train:  2.109914  loss avg: 2.182589   [epoch   0:  4104/ 7691]\t\tt/it 0.12\tETA 2:05:47.519994\n",
            "train:  2.036774  loss avg: 2.181953   [epoch   0:  4122/ 7691]\t\tt/it 0.12\tETA 2:05:44.140658\n",
            "train:  2.062495  loss avg: 2.181433   [epoch   0:  4140/ 7691]\t\tt/it 0.12\tETA 2:05:39.336832\n",
            "train:  1.992010  loss avg: 2.180613   [epoch   0:  4158/ 7691]\t\tt/it 0.12\tETA 2:05:39.728233\n",
            "train:  1.900777  loss avg: 2.179407   [epoch   0:  4176/ 7691]\t\tt/it 0.12\tETA 2:05:52.857046\n",
            "train:  2.077764  loss avg: 2.178971   [epoch   0:  4194/ 7691]\t\tt/it 0.12\tETA 2:05:56.036206\n",
            "train:  1.874264  loss avg: 2.177669   [epoch   0:  4212/ 7691]\t\tt/it 0.12\tETA 2:05:53.379635\n",
            "train:  1.860114  loss avg: 2.176317   [epoch   0:  4230/ 7691]\t\tt/it 0.12\tETA 2:05:48.711311\n",
            "train:  1.956388  loss avg: 2.175385   [epoch   0:  4248/ 7691]\t\tt/it 0.12\tETA 2:06:06.719134\n",
            "train:  1.901361  loss avg: 2.174229   [epoch   0:  4266/ 7691]\t\tt/it 0.12\tETA 2:06:11.397139\n",
            "train:  2.056727  loss avg: 2.173735   [epoch   0:  4284/ 7691]\t\tt/it 0.12\tETA 2:06:07.229893\n",
            "train:  1.954225  loss avg: 2.172817   [epoch   0:  4302/ 7691]\t\tt/it 0.12\tETA 2:06:05.352048\n",
            "train:  1.907077  loss avg: 2.171710   [epoch   0:  4320/ 7691]\t\tt/it 0.12\tETA 2:06:00.711460\n",
            "train:  2.350291  loss avg: 2.172451   [epoch   0:  4338/ 7691]\t\tt/it 0.12\tETA 2:06:19.414920\n",
            "train:  2.054488  loss avg: 2.171963   [epoch   0:  4356/ 7691]\t\tt/it 0.12\tETA 2:06:16.004259\n",
            "train:  2.337195  loss avg: 2.172643   [epoch   0:  4374/ 7691]\t\tt/it 0.12\tETA 2:06:10.976275\n",
            "train:  2.399574  loss avg: 2.173573   [epoch   0:  4392/ 7691]\t\tt/it 0.12\tETA 2:06:08.156195\n",
            "train:  2.005406  loss avg: 2.172887   [epoch   0:  4410/ 7691]\t\tt/it 0.12\tETA 2:06:17.879650\n",
            "train:  1.979719  loss avg: 2.172102   [epoch   0:  4428/ 7691]\t\tt/it 0.12\tETA 2:06:14.633353\n",
            "train:  1.879302  loss avg: 2.170916   [epoch   0:  4446/ 7691]\t\tt/it 0.12\tETA 2:06:13.536092\n",
            "train:  1.860193  loss avg: 2.169663   [epoch   0:  4464/ 7691]\t\tt/it 0.12\tETA 2:06:10.689437\n",
            "train:  2.058615  loss avg: 2.169217   [epoch   0:  4482/ 7691]\t\tt/it 0.12\tETA 2:06:05.105116\n",
            "train:  2.138564  loss avg: 2.169095   [epoch   0:  4500/ 7691]\t\tt/it 0.12\tETA 2:06:13.865340\n",
            "train:  2.203362  loss avg: 2.169231   [epoch   0:  4518/ 7691]\t\tt/it 0.12\tETA 2:06:12.324357\n",
            "train:  2.355008  loss avg: 2.169968   [epoch   0:  4536/ 7691]\t\tt/it 0.12\tETA 2:06:07.481039\n",
            "train:  2.185719  loss avg: 2.170031   [epoch   0:  4554/ 7691]\t\tt/it 0.12\tETA 2:06:09.682483\n",
            "train:  1.986176  loss avg: 2.169307   [epoch   0:  4572/ 7691]\t\tt/it 0.12\tETA 2:06:23.501163\n",
            "train:  2.130878  loss avg: 2.169156   [epoch   0:  4590/ 7691]\t\tt/it 0.12\tETA 2:06:18.682118\n",
            "train:  1.905039  loss avg: 2.168125   [epoch   0:  4608/ 7691]\t\tt/it 0.12\tETA 2:06:17.799618\n",
            "train:  2.119448  loss avg: 2.167935   [epoch   0:  4626/ 7691]\t\tt/it 0.12\tETA 2:06:15.291494\n",
            "train:  2.103972  loss avg: 2.167687   [epoch   0:  4644/ 7691]\t\tt/it 0.12\tETA 2:06:13.057116\n",
            "train:  1.809044  loss avg: 2.166302   [epoch   0:  4662/ 7691]\t\tt/it 0.12\tETA 2:06:24.850926\n",
            "train:  1.862988  loss avg: 2.165136   [epoch   0:  4680/ 7691]\t\tt/it 0.12\tETA 2:06:22.282312\n",
            "train:  2.293578  loss avg: 2.165628   [epoch   0:  4698/ 7691]\t\tt/it 0.12\tETA 2:06:15.800409\n",
            "train:  2.058953  loss avg: 2.165221   [epoch   0:  4716/ 7691]\t\tt/it 0.12\tETA 2:06:10.902560\n",
            "train:  2.040004  loss avg: 2.164745   [epoch   0:  4734/ 7691]\t\tt/it 0.12\tETA 2:06:05.875088\n",
            "train:  2.001183  loss avg: 2.164125   [epoch   0:  4752/ 7691]\t\tt/it 0.12\tETA 2:06:13.957557\n",
            "train:  2.028738  loss avg: 2.163614   [epoch   0:  4770/ 7691]\t\tt/it 0.12\tETA 2:06:09.361666\n",
            "train:  1.938845  loss avg: 2.162769   [epoch   0:  4788/ 7691]\t\tt/it 0.12\tETA 2:06:04.903817\n",
            "train:  2.040313  loss avg: 2.162311   [epoch   0:  4806/ 7691]\t\tt/it 0.12\tETA 2:05:56.902827\n",
            "train:  2.242690  loss avg: 2.162611   [epoch   0:  4824/ 7691]\t\tt/it 0.12\tETA 2:06:04.394529\n",
            "train:  2.097941  loss avg: 2.162370   [epoch   0:  4842/ 7691]\t\tt/it 0.12\tETA 2:05:59.250236\n",
            "train:  2.067521  loss avg: 2.162019   [epoch   0:  4860/ 7691]\t\tt/it 0.12\tETA 2:05:51.600382\n",
            "train:  2.000561  loss avg: 2.161423   [epoch   0:  4878/ 7691]\t\tt/it 0.12\tETA 2:05:47.244540\n",
            "train:  2.046677  loss avg: 2.161001   [epoch   0:  4896/ 7691]\t\tt/it 0.12\tETA 2:05:39.004013\n",
            "train:  2.194042  loss avg: 2.161122   [epoch   0:  4914/ 7691]\t\tt/it 0.12\tETA 2:05:45.765774\n",
            "train:  1.990359  loss avg: 2.160499   [epoch   0:  4932/ 7691]\t\tt/it 0.12\tETA 2:05:38.253556\n",
            "train:  1.909143  loss avg: 2.159585   [epoch   0:  4950/ 7691]\t\tt/it 0.12\tETA 2:05:33.125817\n",
            "train:  2.113332  loss avg: 2.159417   [epoch   0:  4968/ 7691]\t\tt/it 0.12\tETA 2:05:25.969758\n",
            "train:  2.036636  loss avg: 2.158974   [epoch   0:  4986/ 7691]\t\tt/it 0.12\tETA 2:05:32.642525\n",
            "train:  2.166657  loss avg: 2.159002   [epoch   0:  5004/ 7691]\t\tt/it 0.12\tETA 2:05:26.569119\n",
            "train:  2.133332  loss avg: 2.158910   [epoch   0:  5022/ 7691]\t\tt/it 0.12\tETA 2:05:21.596376\n",
            "train:  1.994942  loss avg: 2.158324   [epoch   0:  5040/ 7691]\t\tt/it 0.12\tETA 2:05:17.056382\n",
            "train:  1.947307  loss avg: 2.157573   [epoch   0:  5058/ 7691]\t\tt/it 0.12\tETA 2:05:11.600171\n",
            "train:  1.925119  loss avg: 2.156749   [epoch   0:  5076/ 7691]\t\tt/it 0.12\tETA 2:05:16.486254\n",
            "train:  1.870357  loss avg: 2.155737   [epoch   0:  5094/ 7691]\t\tt/it 0.12\tETA 2:05:20.666358\n",
            "train:  1.867251  loss avg: 2.154721   [epoch   0:  5112/ 7691]\t\tt/it 0.12\tETA 2:05:15.958479\n",
            "train:  1.906727  loss avg: 2.153851   [epoch   0:  5130/ 7691]\t\tt/it 0.12\tETA 2:05:11.858917\n",
            "train:  1.702900  loss avg: 2.152274   [epoch   0:  5148/ 7691]\t\tt/it 0.12\tETA 2:05:20.222246\n",
            "train:  1.977367  loss avg: 2.151665   [epoch   0:  5166/ 7691]\t\tt/it 0.12\tETA 2:05:16.282147\n",
            "train:  2.482295  loss avg: 2.152813   [epoch   0:  5184/ 7691]\t\tt/it 0.12\tETA 2:05:10.447638\n",
            "train:  2.172054  loss avg: 2.152879   [epoch   0:  5202/ 7691]\t\tt/it 0.12\tETA 2:05:06.827678\n",
            "train:  2.024293  loss avg: 2.152436   [epoch   0:  5220/ 7691]\t\tt/it 0.12\tETA 2:05:01.278450\n",
            "train:  1.865618  loss avg: 2.151450   [epoch   0:  5238/ 7691]\t\tt/it 0.12\tETA 2:05:09.771550\n",
            "train:  2.326303  loss avg: 2.152049   [epoch   0:  5256/ 7691]\t\tt/it 0.12\tETA 2:05:05.183686\n",
            "train:  1.908041  loss avg: 2.151216   [epoch   0:  5274/ 7691]\t\tt/it 0.12\tETA 2:05:00.088307\n",
            "train:  1.750674  loss avg: 2.149854   [epoch   0:  5292/ 7691]\t\tt/it 0.12\tETA 2:04:56.509695\n",
            "train:  2.079304  loss avg: 2.149615   [epoch   0:  5310/ 7691]\t\tt/it 0.12\tETA 2:04:51.075033\n",
            "train:  2.012247  loss avg: 2.149151   [epoch   0:  5328/ 7691]\t\tt/it 0.12\tETA 2:04:57.579256\n",
            "train:  1.812253  loss avg: 2.148016   [epoch   0:  5346/ 7691]\t\tt/it 0.12\tETA 2:04:52.878997\n",
            "train:  2.162588  loss avg: 2.148065   [epoch   0:  5364/ 7691]\t\tt/it 0.12\tETA 2:04:54.464036\n",
            "train:  1.927176  loss avg: 2.147327   [epoch   0:  5382/ 7691]\t\tt/it 0.12\tETA 2:04:50.066920\n",
            "train:  2.159157  loss avg: 2.147366   [epoch   0:  5400/ 7691]\t\tt/it 0.12\tETA 2:04:57.939191\n",
            "train:  1.815653  loss avg: 2.146264   [epoch   0:  5418/ 7691]\t\tt/it 0.12\tETA 2:04:53.259339\n",
            "train:  2.347035  loss avg: 2.146929   [epoch   0:  5436/ 7691]\t\tt/it 0.12\tETA 2:04:47.649619\n",
            "train:  1.723597  loss avg: 2.145532   [epoch   0:  5454/ 7691]\t\tt/it 0.12\tETA 2:04:42.547177\n",
            "train:  2.056037  loss avg: 2.145237   [epoch   0:  5472/ 7691]\t\tt/it 0.12\tETA 2:04:38.605405\n",
            "train:  1.870164  loss avg: 2.144335   [epoch   0:  5490/ 7691]\t\tt/it 0.12\tETA 2:04:46.843421\n",
            "train:  1.917307  loss avg: 2.143593   [epoch   0:  5508/ 7691]\t\tt/it 0.12\tETA 2:04:42.359202\n",
            "train:  2.135001  loss avg: 2.143565   [epoch   0:  5526/ 7691]\t\tt/it 0.12\tETA 2:04:39.693950\n",
            "train:  2.331296  loss avg: 2.144175   [epoch   0:  5544/ 7691]\t\tt/it 0.12\tETA 2:04:36.937457\n",
            "train:  2.079137  loss avg: 2.143965   [epoch   0:  5562/ 7691]\t\tt/it 0.12\tETA 2:04:44.734940\n",
            "train:  1.859416  loss avg: 2.143047   [epoch   0:  5580/ 7691]\t\tt/it 0.12\tETA 2:04:41.145433\n",
            "train:  2.150462  loss avg: 2.143070   [epoch   0:  5598/ 7691]\t\tt/it 0.12\tETA 2:04:34.459217\n",
            "train:  1.848020  loss avg: 2.142125   [epoch   0:  5616/ 7691]\t\tt/it 0.12\tETA 2:04:29.305814\n",
            "train:  1.808630  loss avg: 2.141059   [epoch   0:  5634/ 7691]\t\tt/it 0.12\tETA 2:04:23.936368\n",
            "train:  2.249668  loss avg: 2.141405   [epoch   0:  5652/ 7691]\t\tt/it 0.12\tETA 2:04:30.593203\n",
            "train:  1.882302  loss avg: 2.140583   [epoch   0:  5670/ 7691]\t\tt/it 0.12\tETA 2:04:25.722510\n",
            "train:  2.012595  loss avg: 2.140178   [epoch   0:  5688/ 7691]\t\tt/it 0.12\tETA 2:04:19.634546\n",
            "train:  2.129728  loss avg: 2.140145   [epoch   0:  5706/ 7691]\t\tt/it 0.12\tETA 2:04:13.386151\n",
            "train:  2.053940  loss avg: 2.139874   [epoch   0:  5724/ 7691]\t\tt/it 0.12\tETA 2:04:20.060688\n",
            "train:  1.953862  loss avg: 2.139290   [epoch   0:  5742/ 7691]\t\tt/it 0.12\tETA 2:04:15.998677\n",
            "train:  1.974244  loss avg: 2.138775   [epoch   0:  5760/ 7691]\t\tt/it 0.12\tETA 2:04:09.841929\n",
            "train:  2.026975  loss avg: 2.138426   [epoch   0:  5778/ 7691]\t\tt/it 0.12\tETA 2:04:06.468709\n",
            "train:  1.958486  loss avg: 2.137868   [epoch   0:  5796/ 7691]\t\tt/it 0.12\tETA 2:04:01.928799\n",
            "train:  2.115895  loss avg: 2.137800   [epoch   0:  5814/ 7691]\t\tt/it 0.12\tETA 2:04:06.419246\n",
            "train:  2.275907  loss avg: 2.138226   [epoch   0:  5832/ 7691]\t\tt/it 0.12\tETA 2:03:59.685563\n",
            "train:  1.783379  loss avg: 2.137134   [epoch   0:  5850/ 7691]\t\tt/it 0.12\tETA 2:03:56.344402\n",
            "train:  2.259720  loss avg: 2.137510   [epoch   0:  5868/ 7691]\t\tt/it 0.12\tETA 2:03:55.765608\n",
            "train:  2.122827  loss avg: 2.137465   [epoch   0:  5886/ 7691]\t\tt/it 0.12\tETA 2:04:02.493799\n",
            "train:  1.867171  loss avg: 2.136641   [epoch   0:  5904/ 7691]\t\tt/it 0.12\tETA 2:03:59.131518\n",
            "train:  2.061307  loss avg: 2.136412   [epoch   0:  5922/ 7691]\t\tt/it 0.12\tETA 2:03:56.045492\n",
            "train:  2.036418  loss avg: 2.136109   [epoch   0:  5940/ 7691]\t\tt/it 0.12\tETA 2:03:51.520718\n",
            "train:  1.718963  loss avg: 2.134849   [epoch   0:  5958/ 7691]\t\tt/it 0.12\tETA 2:03:47.892621\n",
            "train:  1.833830  loss avg: 2.133942   [epoch   0:  5976/ 7691]\t\tt/it 0.12\tETA 2:03:55.811768\n",
            "train:  1.757106  loss avg: 2.132810   [epoch   0:  5994/ 7691]\t\tt/it 0.12\tETA 2:03:52.575277\n",
            "train:  2.071020  loss avg: 2.132625   [epoch   0:  6012/ 7691]\t\tt/it 0.12\tETA 2:03:50.452759\n",
            "train:  1.819734  loss avg: 2.131691   [epoch   0:  6030/ 7691]\t\tt/it 0.12\tETA 2:03:46.363232\n",
            "train:  1.775031  loss avg: 2.130630   [epoch   0:  6048/ 7691]\t\tt/it 0.12\tETA 2:03:46.599516\n",
            "train:  2.116922  loss avg: 2.130589   [epoch   0:  6066/ 7691]\t\tt/it 0.12\tETA 2:03:52.605302\n",
            "train:  2.063160  loss avg: 2.130390   [epoch   0:  6084/ 7691]\t\tt/it 0.12\tETA 2:03:49.161063\n",
            "train:  2.474161  loss avg: 2.131404   [epoch   0:  6102/ 7691]\t\tt/it 0.12\tETA 2:03:43.800794\n",
            "train:  2.146919  loss avg: 2.131450   [epoch   0:  6120/ 7691]\t\tt/it 0.12\tETA 2:03:40.821174\n",
            "train:  2.036317  loss avg: 2.131171   [epoch   0:  6138/ 7691]\t\tt/it 0.12\tETA 2:03:45.931797\n",
            "train:  2.118529  loss avg: 2.131134   [epoch   0:  6156/ 7691]\t\tt/it 0.12\tETA 2:03:40.137571\n",
            "train:  2.262874  loss avg: 2.131518   [epoch   0:  6174/ 7691]\t\tt/it 0.12\tETA 2:03:36.755722\n",
            "train:  1.984388  loss avg: 2.131090   [epoch   0:  6192/ 7691]\t\tt/it 0.12\tETA 2:03:36.916758\n",
            "train:  2.074050  loss avg: 2.130925   [epoch   0:  6210/ 7691]\t\tt/it 0.12\tETA 2:03:33.873931\n",
            "train:  1.848031  loss avg: 2.130107   [epoch   0:  6228/ 7691]\t\tt/it 0.12\tETA 2:03:41.622862\n",
            "train:  1.789828  loss avg: 2.129126   [epoch   0:  6246/ 7691]\t\tt/it 0.12\tETA 2:03:36.782774\n",
            "train:  2.147248  loss avg: 2.129178   [epoch   0:  6264/ 7691]\t\tt/it 0.12\tETA 2:03:34.502639\n",
            "train:  2.348006  loss avg: 2.129805   [epoch   0:  6282/ 7691]\t\tt/it 0.12\tETA 2:03:30.835218\n",
            "train:  2.062656  loss avg: 2.129614   [epoch   0:  6300/ 7691]\t\tt/it 0.12\tETA 2:03:36.375166\n",
            "train:  2.349211  loss avg: 2.130239   [epoch   0:  6318/ 7691]\t\tt/it 0.12\tETA 2:03:32.082401\n",
            "train:  2.202602  loss avg: 2.130445   [epoch   0:  6336/ 7691]\t\tt/it 0.12\tETA 2:03:27.795723\n",
            "train:  2.215570  loss avg: 2.130686   [epoch   0:  6354/ 7691]\t\tt/it 0.12\tETA 2:03:23.878907\n",
            "train:  2.106390  loss avg: 2.130617   [epoch   0:  6372/ 7691]\t\tt/it 0.12\tETA 2:03:23.331593\n",
            "train:  1.907461  loss avg: 2.129989   [epoch   0:  6390/ 7691]\t\tt/it 0.12\tETA 2:03:30.909994\n",
            "train:  2.172542  loss avg: 2.130108   [epoch   0:  6408/ 7691]\t\tt/it 0.12\tETA 2:03:30.767301\n",
            "train:  1.838738  loss avg: 2.129292   [epoch   0:  6426/ 7691]\t\tt/it 0.12\tETA 2:03:30.958945\n",
            "train:  2.251231  loss avg: 2.129633   [epoch   0:  6444/ 7691]\t\tt/it 0.12\tETA 2:03:33.859495\n",
            "train:  1.668033  loss avg: 2.128347   [epoch   0:  6462/ 7691]\t\tt/it 0.12\tETA 2:03:45.256111\n",
            "train:  2.223777  loss avg: 2.128612   [epoch   0:  6480/ 7691]\t\tt/it 0.12\tETA 2:03:46.127980\n",
            "train:  2.086104  loss avg: 2.128494   [epoch   0:  6498/ 7691]\t\tt/it 0.12\tETA 2:03:48.595647\n",
            "train:  1.842051  loss avg: 2.127703   [epoch   0:  6516/ 7691]\t\tt/it 0.12\tETA 2:03:47.370891\n",
            "train:  2.046168  loss avg: 2.127478   [epoch   0:  6534/ 7691]\t\tt/it 0.12\tETA 2:03:44.300896\n",
            "train:  2.035577  loss avg: 2.127226   [epoch   0:  6552/ 7691]\t\tt/it 0.12\tETA 2:03:55.333755\n",
            "train:  2.099952  loss avg: 2.127151   [epoch   0:  6570/ 7691]\t\tt/it 0.12\tETA 2:03:51.893898\n",
            "train:  2.067922  loss avg: 2.126989   [epoch   0:  6588/ 7691]\t\tt/it 0.12\tETA 2:03:48.456337\n",
            "train:  1.837908  loss avg: 2.126202   [epoch   0:  6606/ 7691]\t\tt/it 0.12\tETA 2:03:42.939705\n",
            "train:  1.901858  loss avg: 2.125592   [epoch   0:  6624/ 7691]\t\tt/it 0.12\tETA 2:03:49.260179\n",
            "train:  1.807503  loss avg: 2.124730   [epoch   0:  6642/ 7691]\t\tt/it 0.12\tETA 2:03:46.286326\n",
            "train:  1.957738  loss avg: 2.124279   [epoch   0:  6660/ 7691]\t\tt/it 0.12\tETA 2:03:44.156415\n",
            "train:  2.262177  loss avg: 2.124650   [epoch   0:  6678/ 7691]\t\tt/it 0.12\tETA 2:03:40.095628\n",
            "train:  2.393231  loss avg: 2.125372   [epoch   0:  6696/ 7691]\t\tt/it 0.12\tETA 2:03:35.236170\n",
            "train:  2.050442  loss avg: 2.125171   [epoch   0:  6714/ 7691]\t\tt/it 0.12\tETA 2:03:40.870493\n",
            "train:  2.089324  loss avg: 2.125076   [epoch   0:  6732/ 7691]\t\tt/it 0.12\tETA 2:03:36.800318\n",
            "train:  2.392540  loss avg: 2.125789   [epoch   0:  6750/ 7691]\t\tt/it 0.12\tETA 2:03:34.310961\n",
            "train:  2.074157  loss avg: 2.125652   [epoch   0:  6768/ 7691]\t\tt/it 0.12\tETA 2:03:30.663537\n",
            "train:  1.771932  loss avg: 2.124713   [epoch   0:  6786/ 7691]\t\tt/it 0.12\tETA 2:03:36.049515\n",
            "train:  2.038898  loss avg: 2.124486   [epoch   0:  6804/ 7691]\t\tt/it 0.12\tETA 2:03:31.137829\n",
            "train:  1.838116  loss avg: 2.123731   [epoch   0:  6822/ 7691]\t\tt/it 0.12\tETA 2:03:27.830822\n",
            "train:  2.241813  loss avg: 2.124041   [epoch   0:  6840/ 7691]\t\tt/it 0.12\tETA 2:03:23.380899\n",
            "train:  2.074228  loss avg: 2.123911   [epoch   0:  6858/ 7691]\t\tt/it 0.12\tETA 2:03:19.986169\n",
            "train:  2.108401  loss avg: 2.123870   [epoch   0:  6876/ 7691]\t\tt/it 0.12\tETA 2:03:28.305882\n",
            "train:  2.160728  loss avg: 2.123966   [epoch   0:  6894/ 7691]\t\tt/it 0.12\tETA 2:03:24.555506\n",
            "train:  1.750954  loss avg: 2.122995   [epoch   0:  6912/ 7691]\t\tt/it 0.12\tETA 2:03:20.760697\n",
            "train:  2.191700  loss avg: 2.123173   [epoch   0:  6930/ 7691]\t\tt/it 0.12\tETA 2:03:17.354139\n",
            "train:  2.463224  loss avg: 2.124054   [epoch   0:  6948/ 7691]\t\tt/it 0.12\tETA 2:03:22.193698\n",
            "train:  1.923398  loss avg: 2.123536   [epoch   0:  6966/ 7691]\t\tt/it 0.12\tETA 2:03:19.267315\n",
            "train:  2.676457  loss avg: 2.124961   [epoch   0:  6984/ 7691]\t\tt/it 0.12\tETA 2:03:20.817825\n",
            "train:  2.079983  loss avg: 2.124845   [epoch   0:  7002/ 7691]\t\tt/it 0.12\tETA 2:03:15.116064\n",
            "train:  1.977906  loss avg: 2.124468   [epoch   0:  7020/ 7691]\t\tt/it 0.12\tETA 2:03:12.609284\n",
            "train:  2.096464  loss avg: 2.124397   [epoch   0:  7038/ 7691]\t\tt/it 0.12\tETA 2:03:18.913981\n",
            "train:  2.023520  loss avg: 2.124140   [epoch   0:  7056/ 7691]\t\tt/it 0.12\tETA 2:03:15.338865\n",
            "train:  1.990982  loss avg: 2.123801   [epoch   0:  7074/ 7691]\t\tt/it 0.12\tETA 2:03:09.929451\n",
            "train:  1.775978  loss avg: 2.122918   [epoch   0:  7092/ 7691]\t\tt/it 0.12\tETA 2:03:07.279846\n",
            "train:  2.050983  loss avg: 2.122736   [epoch   0:  7110/ 7691]\t\tt/it 0.12\tETA 2:03:12.752267\n",
            "train:  2.046994  loss avg: 2.122545   [epoch   0:  7128/ 7691]\t\tt/it 0.12\tETA 2:03:08.227898\n",
            "train:  1.848293  loss avg: 2.121854   [epoch   0:  7146/ 7691]\t\tt/it 0.12\tETA 2:03:04.783460\n",
            "train:  2.126840  loss avg: 2.121866   [epoch   0:  7164/ 7691]\t\tt/it 0.12\tETA 2:03:02.081118\n",
            "train:  2.140784  loss avg: 2.121914   [epoch   0:  7182/ 7691]\t\tt/it 0.12\tETA 2:02:57.146833\n",
            "train:  2.200526  loss avg: 2.122110   [epoch   0:  7200/ 7691]\t\tt/it 0.12\tETA 2:02:59.653803\n",
            "train:  1.900848  loss avg: 2.121558   [epoch   0:  7218/ 7691]\t\tt/it 0.12\tETA 2:02:57.168991\n",
            "train:  1.881236  loss avg: 2.120961   [epoch   0:  7236/ 7691]\t\tt/it 0.12\tETA 2:02:54.490394\n",
            "train:  1.798739  loss avg: 2.120161   [epoch   0:  7254/ 7691]\t\tt/it 0.12\tETA 2:02:50.108427\n",
            "train:  1.876112  loss avg: 2.119557   [epoch   0:  7272/ 7691]\t\tt/it 0.12\tETA 2:02:54.419270\n",
            "train:  1.810356  loss avg: 2.118793   [epoch   0:  7290/ 7691]\t\tt/it 0.12\tETA 2:02:50.048755\n",
            "train:  2.164495  loss avg: 2.118906   [epoch   0:  7308/ 7691]\t\tt/it 0.12\tETA 2:02:46.573628\n",
            "train:  2.136168  loss avg: 2.118948   [epoch   0:  7326/ 7691]\t\tt/it 0.12\tETA 2:02:45.847021\n",
            "train:  2.037705  loss avg: 2.118749   [epoch   0:  7344/ 7691]\t\tt/it 0.12\tETA 2:02:42.605412\n",
            "train:  2.083389  loss avg: 2.118663   [epoch   0:  7362/ 7691]\t\tt/it 0.12\tETA 2:02:47.339349\n",
            "train:  2.046222  loss avg: 2.118486   [epoch   0:  7380/ 7691]\t\tt/it 0.12\tETA 2:02:42.163514\n",
            "train:  2.073256  loss avg: 2.118376   [epoch   0:  7398/ 7691]\t\tt/it 0.12\tETA 2:02:39.450406\n",
            "train:  2.036726  loss avg: 2.118178   [epoch   0:  7416/ 7691]\t\tt/it 0.12\tETA 2:02:35.386564\n",
            "train:  1.942623  loss avg: 2.117753   [epoch   0:  7434/ 7691]\t\tt/it 0.12\tETA 2:02:39.952165\n",
            "train:  2.326447  loss avg: 2.118257   [epoch   0:  7452/ 7691]\t\tt/it 0.12\tETA 2:02:35.795208\n",
            "train:  2.120299  loss avg: 2.118262   [epoch   0:  7470/ 7691]\t\tt/it 0.12\tETA 2:02:32.517722\n",
            "train:  2.030034  loss avg: 2.118050   [epoch   0:  7488/ 7691]\t\tt/it 0.12\tETA 2:02:27.203174\n",
            "train:  1.992659  loss avg: 2.117749   [epoch   0:  7506/ 7691]\t\tt/it 0.12\tETA 2:02:34.880637\n",
            "train:  2.112026  loss avg: 2.117735   [epoch   0:  7524/ 7691]\t\tt/it 0.12\tETA 2:02:30.291418\n",
            "train:  2.235169  loss avg: 2.118016   [epoch   0:  7542/ 7691]\t\tt/it 0.12\tETA 2:02:27.009649\n",
            "train:  2.260459  loss avg: 2.118355   [epoch   0:  7560/ 7691]\t\tt/it 0.12\tETA 2:02:25.576760\n",
            "train:  2.022518  loss avg: 2.118127   [epoch   0:  7578/ 7691]\t\tt/it 0.12\tETA 2:02:23.009751\n",
            "train:  1.970616  loss avg: 2.117778   [epoch   0:  7596/ 7691]\t\tt/it 0.12\tETA 2:02:26.992094\n",
            "train:  2.143744  loss avg: 2.117839   [epoch   0:  7614/ 7691]\t\tt/it 0.12\tETA 2:02:23.840429\n",
            "train:  1.926722  loss avg: 2.117388   [epoch   0:  7632/ 7691]\t\tt/it 0.12\tETA 2:02:19.955716\n",
            "train:  1.920497  loss avg: 2.116925   [epoch   0:  7650/ 7691]\t\tt/it 0.12\tETA 2:02:16.171899\n",
            "train:  2.116886  loss avg: 2.116925   [epoch   0:  7668/ 7691]\t\tt/it 0.12\tETA 2:02:21.302325\n",
            "train:  1.982864  loss avg: 2.116611   [epoch   0:  7686/ 7691]\t\tt/it 0.12\tETA 2:02:18.900757\n",
            "train:  2.123714  loss avg: 2.116628   [epoch   0:  2140/ 7691]\t\tt/it 0.11\tETA 2:02:06.710513\n",
            "train:  2.123714  loss avg: 0.117789   [epoch   0:  7691/ 7691]\t\tt/epoch 100.491370677948\n",
            "        2.123714  accuracy: 0.254063\n",
            "val:    loss avg: 0.114766   [epoch   0]\n",
            "        accuracy: 0.114766\n",
            "\n",
            "Saving current state to '.\\runs/20231218-19h48m28s\\checkpoints\\epoch_0'\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "File .\\runs/20231218-19h48m28s\\checkpoints\\epoch_0 cannot be opened.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m checkpoint_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving current state to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\colin\\OneDrive-TUM\\Code\\envs\\231110_DatScieEO\\lib\\site-packages\\torch\\serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\colin\\OneDrive-TUM\\Code\\envs\\231110_DatScieEO\\lib\\site-packages\\torch\\serialization.py:492\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\colin\\OneDrive-TUM\\Code\\envs\\231110_DatScieEO\\lib\\site-packages\\torch\\serialization.py:463\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: File .\\runs/20231218-19h48m28s\\checkpoints\\epoch_0 cannot be opened."
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "writer = SummaryWriter(run_dir)\n",
        "\n",
        "t0 = time.time()\n",
        "last_info = t0\n",
        "n_train = len(dl_train.dataset)\n",
        "n_batch_train = np.ceil(n_train/batch_size)\n",
        "its_total = n_batch_train * N_epochs\n",
        "n_val = len(dl_val.dataset)\n",
        "n_batch_val = np.ceil(n_val/batch_size)\n",
        "\n",
        "for i_epoch in range(N_epochs):\n",
        "\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "    correct = 0\n",
        "\n",
        "    model.train()\n",
        "    for i_batch, (x, y) in enumerate(dl_train):\n",
        "        assert torch.isnan(x).sum() == 0, \"NaN in trainings data, please fix.\"\n",
        "\n",
        "        x = x.float().to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        pred = model(x)\n",
        "\n",
        "        loss = loss_fn(pred, y)\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "        if verbose and (((time.time() - last_info) > 20) or (i_batch % (n_batch_train//10) == 0)):\n",
        "            loss_avg = running_loss / (i_batch+1) # loss per batch\n",
        "            last_info = time.time()\n",
        "            loss, current = loss.item(), (i_batch + 1) * len(x)\n",
        "            curr_it = i_epoch*n_train + i_batch+1\n",
        "            t_per_it = (time.time()-t0) / curr_it\n",
        "            ETA = (its_total - curr_it) * t_per_it\n",
        "            writer.add_scalar(\"train_intermediate/loss\", loss_avg, i_epoch+current/n_train)\n",
        "            print(f\"train:  {loss:>7f}  loss avg: {loss_avg:>7f}   [epoch {i_epoch:>3d}: {current:>5d}/{n_train:>5d}]\\t\\tt/it {t_per_it:.2f}\\tETA {datetime.timedelta(seconds=ETA)}\")\n",
        "\n",
        "    all = [(el, y_) for x, y in dl_train for el, y_ in zip(np.argmax(model(x.float().to(device)).detach().cpu().numpy(), axis=1), y.detach().cpu().numpy())]\n",
        "    all_preds = [a[0] for a in all]\n",
        "    all_gts = [a[1] for a in all]\n",
        "    accuracy_train = accuracy_score(all_gts, all_preds)\n",
        "\n",
        "    t_per_epoch = (time.time()-t0)/(i_epoch+1)\n",
        "\n",
        "    loss_avg = running_loss / n_train\n",
        "    if verbose: print(f\"train:  {loss:>7f}  loss avg: {loss_avg:>7f}   [epoch {i_epoch:>3d}: {n_train:>5d}/{n_train:>5d}]\\t\\tt/epoch {t_per_epoch}\")\n",
        "    if verbose: print(f\"        {loss:>7f}  accuracy: {accuracy_train:>7f}\")\n",
        "\n",
        "\n",
        "    # validation loop\n",
        "    model.eval()\n",
        "    running_loss_val = 0.\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i_batch_val, (x, y) in enumerate(dl_val):\n",
        "            x = x.float().to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            pred_val = model(x)\n",
        "\n",
        "            pred_class = np.argmax(pred_val.cpu().numpy(), axis=1)\n",
        "            correct == (pred_class == y.cpu().numpy()).sum()\n",
        "\n",
        "            loss_val = loss_fn(pred_val, y)\n",
        "            running_loss_val += loss_val\n",
        "\n",
        "    accuracy_val = correct / n_val\n",
        "    loss_val_avg = running_loss_val / n_val\n",
        "\n",
        "    if verbose: print(f\"val:    loss avg: {loss_val_avg:>7f}   [epoch {i_epoch:>3d}]\")\n",
        "    if verbose: print(f\"        accuracy: {loss_val_avg:>7f}\")\n",
        "\n",
        "    # write some metrics to tensorboard\n",
        "    writer.add_scalars(\"_train_val/loss\", {\"train\": loss_avg, \"val\": loss_val_avg}, i_epoch+1)\n",
        "    writer.add_scalars(\"_train_val/accuracy\", {\"train\": accuracy_train, \"val\": accuracy_val}, i_epoch+1)\n",
        "    writer.add_scalar(\"train/loss\", loss_avg, i_epoch+1)\n",
        "    writer.add_scalar(\"train/accuracy\", accuracy_train, i_epoch+1)\n",
        "    writer.add_scalar(\"val/loss\", loss_val_avg, i_epoch+1)\n",
        "    writer.add_scalar(\"val/accuracy\", accuracy_val, i_epoch+1)\n",
        "\n",
        "    checkpoint_file = os.path.join(checkpoint_dir, f\"epoch_{i_epoch}.pth\")\n",
        "    if verbose: print(f\"Saving current state to '{checkpoint_file}'\\n\")\n",
        "    torch.save({\n",
        "            'epoch': i_epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optim.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, checkpoint_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "231110_DatScieEO",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
