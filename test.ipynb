{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a `torch` model\n",
    "---\n",
    "\n",
    "This notebook provides an implementation of the prediction step with a Pytorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "run_colab = \"google\" in {pkg.key for pkg in pkg_resources.working_set}\n",
    "\n",
    "if run_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "    root = \"/content/drive/MyDrive/DatSciEO\"\n",
    "    import os, sys\n",
    "    sys.path.append(root)\n",
    "\n",
    "    data_dir_in = \"1123_top10\"\n",
    "    data_name = \"1123_delete_nan_samples\"\n",
    "    data_zip = f\"/content/drive/My\\ Drive/DatSciEO/data/{data_dir_in}/{data_name}.zip\"\n",
    "    data_root = \"/content/data/\"\n",
    "    data_out_dir = os.path.join(data_root, data_dir_in)\n",
    "    os.makedirs(data_out_dir, exist_ok=True)\n",
    "    print(f\"unzipping data to {data_out_dir}\")\n",
    "    !unzip -o {data_zip} -d {data_out_dir}\n",
    "    print(\"unzipping done\")\n",
    "else:\n",
    "    root = \".\"\n",
    "    data_root = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from models import TreeClassifConvNet, TreeClassifResNet50\n",
    "from utils import TreeClassifPreprocessedDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general training settings\n",
    "batch_size_test = 20\n",
    "verbose = True\n",
    "\n",
    "checkpoint = torch.load(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using dataset with properties:\n",
      "\tsamples:    54930\n",
      "\t   train:   38451\n",
      "\t   val:     16479\n",
      "\tshape: (30, 5, 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create datasets and dataloaders\n",
    "dataset_dir = os.path.join(data_root, \"data/1123_top10/1123_delete_nan_samples\")\n",
    "ds_test = TreeClassifPreprocessedDataset(dataset_dir, indices=range(100, 200))\n",
    "dl_test = DataLoader(ds_test, batch_size_test, shuffle=True)\n",
    "\n",
    "# define dataloaders for training\n",
    "\n",
    "if verbose: print(\n",
    "    f\"\\nUsing dataset with properties:\\n\"       \\\n",
    "    f\"\\tsamples test:    {len(ds_test)}\\n\"      \\\n",
    "    f\"\\tshape: {ds_test[0][0].shape}\\n\"         \\\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeClassifConvNet(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(30, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(15, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(7, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=125, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "with 6597 parameters\n"
     ]
    }
   ],
   "source": [
    "# model, loss and optimizer\n",
    "model = TreeClassifResNet50(\n",
    "    n_classes = ds_test.n_classes,\n",
    "    width = ds_test.width,\n",
    "    height = ds_test.height,\n",
    "    depth = ds_test.depth\n",
    ")\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "if verbose:\n",
    "    print(model)\n",
    "    print(f\"with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch_test = np.ceil(len(ds_test)/batch_size_test)\n",
    "\n",
    "# prediction loop\n",
    "correct = 0\n",
    "last_info = time.time()\n",
    "t0 = time.time()\n",
    "model.eval()\n",
    "all_gts = []\n",
    "all_preds = []\n",
    "for i_batch, (x, y) in enumerate(dl_test):\n",
    "    assert torch.isnan(x).sum() == 0, \"NaN in test data, please fix.\"\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    pred = model(x)\n",
    "\n",
    "    all_gts.extend(y.detach().cpu().numpy())\n",
    "    all_preds.extend(np.argmax(pred.detach().cpu().numpy(), axis=1))\n",
    "\n",
    "\n",
    "    if verbose and (((time.time() - last_info) > 20) or (i_batch % (n_batch_test//10) == 0)):\n",
    "        last_info = time.time()\n",
    "        loss, current = loss.item(), (i_batch + 1) * len(x)\n",
    "        t_per_it = (time.time()-t0) / (i_batch+1)\n",
    "        ETA = (n_batch_test - i_batch - 1) * t_per_it\n",
    "        print(f\"train:  {loss:>7f}     [{current:>5d}/{len(ds_test):>5d}]\\t\\tt/it {t_per_it:.2f}\\tETA {datetime.timedelta(seconds=ETA)}\\t{(datetime.datetime.now() + datetime.timedelta(seconds=ETA)).strftime('%Y-%m-%d %Hh%Mm%Ss')}\")\n",
    "\n",
    "accuracy_test = accuracy_score(all_gts, all_preds)\n",
    "# TODO: confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
